{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178b68ac-c50a-4220-bd51-8244b5b830d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "\n",
    "# Third-party imports\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "\n",
    "# PySpark core\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import *\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (col, lit, when, expr, trim, upper, isnull, broadcast,\n",
    "    substring, length, concat_ws, regexp_replace, regexp_extract, greatest, lower,\n",
    "    coalesce, soundex, levenshtein\n",
    ")\n",
    "\n",
    "# PySpark types\n",
    "from pyspark.sql.types import (StringType, IntegerType, StructType, StructField, \n",
    "    BooleanType, FloatType\n",
    ")\n",
    "\n",
    "# UDF\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653692e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark with optimizations for large datasets\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3a4cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imput rows: 10208188\n"
     ]
    }
   ],
   "source": [
    "# Define dynamic paths\n",
    "input_pattern = \"/mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Data/ndsu_split_file_*.csv\"\n",
    "sic_code_path = \"/mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Agustin/cleaned_SIC_code_list.csv\"\n",
    "cities_path = \"/mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Agustin/cities.csv\"\n",
    "fortune_path = \"/mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Agustin/fortune1000_2024.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "df = spark.read.csv(input_pattern, header=True, inferSchema=True)\n",
    "sic_code_df = spark.read.csv(sic_code_path, header=True, inferSchema=True)\n",
    "cities_df = spark.read.csv(cities_path, header=True, inferSchema=True)\n",
    "\n",
    "# Load and prepare Fortune table with Company and Alt_name\n",
    "fortune_df = spark.read.csv(fortune_path, header=True) \\\n",
    "    .select(\n",
    "        col(\"Company\").alias(\"fortune_company\"),\n",
    "        col(\"Alt_name\").alias(\"fortune_alt_name\")\n",
    "    ) \\\n",
    "    .filter(col(\"fortune_company\").isNotNull()) \\\n",
    "    .distinct() \\\n",
    "    .cache()\n",
    "\n",
    "\n",
    "print(f\"Total imput rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd75684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation sets for geographic validation\n",
    "valid_provinces = {'AB', 'BC', 'MB', 'NB', 'NL', 'NS', 'NT', 'NU', 'ON', 'PE', 'QC', 'SK', 'YT'}\n",
    "valid_states = {\n",
    "    'PR', 'DC', 'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "}\n",
    "valid_countries = {\n",
    "    'AF', 'AL', 'DZ', 'AS', 'AD', 'AO', 'AI', 'AQ', 'AG', 'AR', \n",
    "    'AM', 'AW', 'AU', 'AT', 'AZ', 'BS', 'BH', 'BD', 'BB', 'BY', \n",
    "    'BE', 'BZ', 'BJ', 'BM', 'BT', 'BO', 'BA', 'BW', 'BV', 'BR', \n",
    "    'IO', 'BN', 'BG', 'BF', 'BI', 'CV', 'KH', 'CM', 'CA', 'KY', \n",
    "    'CF', 'TD', 'CL', 'CN', 'CX', 'CC', 'CO', 'KM', 'CG', 'CD', \n",
    "    'CK', 'CR', 'CI', 'HR', 'CU', 'CY', 'CZ', 'DK', 'DJ', 'DM', \n",
    "    'DO', 'EC', 'EG', 'SV', 'GQ', 'ER', 'EE', 'ET', 'FK', 'FO', \n",
    "    'FJ', 'FI', 'FR', 'GF', 'PF', 'TF', 'GA', 'GM', 'GE', 'DE', \n",
    "    'GH', 'GI', 'GR', 'GL', 'GD', 'GP', 'GU', 'GT', 'GG', 'GN', \n",
    "    'GW', 'GY', 'HT', 'HM', 'VA', 'HN', 'HK', 'HU', 'IS', 'IN', \n",
    "    'ID', 'IR', 'IQ', 'IE', 'IM', 'IL', 'IT', 'JM', 'JP', 'JE', \n",
    "    'JO', 'KZ', 'KE', 'KI', 'KP', 'KR', 'KW', 'KG', 'LA', 'LV', \n",
    "    'LB', 'LS', 'LR', 'LY', 'LI', 'LT', 'LU', 'MO', 'MK', 'MG', \n",
    "    'MW', 'MY', 'MV', 'ML', 'MT', 'MH', 'MQ', 'MR', 'MU', 'YT', \n",
    "    'MX', 'FM', 'MD', 'MC', 'MN', 'ME', 'MS', 'MA', 'MZ', 'MM', \n",
    "    'NA', 'NR', 'NP', 'NL', 'NC', 'NZ', 'NI', 'NE', 'NG', 'NU', \n",
    "    'NF', 'MP', 'NO', 'OM', 'PK', 'PW', 'PS', 'PA', 'PG', 'PY', \n",
    "    'PE', 'PH', 'PN', 'PL', 'PT', 'PR', 'QA', 'RE', 'RO', 'RU', \n",
    "    'RW', 'BL', 'SH', 'KN', 'LC', 'MF', 'PM', 'VC', 'WS', 'SM', \n",
    "    'ST', 'SA', 'SN', 'RS', 'SC', 'SL', 'SG', 'SX', 'SK', 'SI', \n",
    "    'SB', 'SO', 'ZA', 'GS', 'SS', 'ES', 'LK', 'SD', 'SR', 'SJ', \n",
    "    'SZ', 'SE', 'CH', 'SY', 'TW', 'TJ', 'TZ', 'TH', 'TL', 'TG', \n",
    "    'TK', 'TO', 'TT', 'TN', 'TR', 'TM', 'TC', 'TV', 'UG', 'UA', \n",
    "    'AE', 'GB', 'US', 'UM', 'UY', 'UZ', 'VU', 'VE', 'VN', 'VG', \n",
    "    'VI', 'WF', 'EH', 'YE', 'ZM', 'ZW'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b1a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize abbreviations (e.g., \"ST.\" â†’ \"ST\")\n",
    "abbrev_replacements = {\n",
    "    r\"\\bST\\.\\b\": \"ST\",\n",
    "    r\"\\bRD\\.\\b\": \"RD\",\n",
    "    r\"\\bAVE\\.\\b\": \"AVE\",\n",
    "    r\"\\bBLVD\\.\\b\": \"BLVD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7387fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 10208188\n"
     ]
    }
   ],
   "source": [
    "# 1. Merge with SIC codes\n",
    "deduped_sic = sic_code_df.dropDuplicates([\"Related SIC Code\"])\n",
    "\n",
    "merged_df = df.join(\n",
    "    deduped_sic.select(\n",
    "        col(\"Related SIC Code\").alias(\"SICSUBCD\"), \n",
    "        col(\"Related SIC Code Description\")\n",
    "    ),\n",
    "    on=\"SICSUBCD\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"Total rows: {merged_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8da937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Add Online_status (1/0)\n",
    "# Define online-exclusive SIC codes (4-digit)\n",
    "online_sic_codes = {\n",
    "    # Software/Internet Services\n",
    "    \"7371\", \"7372\", \"7373\", \"7374\", \"7375\", \"7376\", \"7377\", \"7378\", \"7379\",\n",
    "    \n",
    "    # E-Commerce/Digital Content\n",
    "    \"5961\",  # Mail-order/catalog retail\n",
    "    \"5962\",  # Automatic merchandising (digital vending)\n",
    "    \"5944\",  # Computer software stores (digital distribution)\n",
    "    \"4841\",  # Video streaming\n",
    "    \"7822\",  # Motion picture and video distribution\n",
    "    \n",
    "    # Digital Financial Services\n",
    "    \"6099\",  # Cryptocurrency exchanges\n",
    "    \"6211\",  # Online brokerages\n",
    "    \n",
    "    # Telecommunications\n",
    "    \"4812\",  # Mobile apps\n",
    "    \"4813\",  # VoIP services\n",
    "    \n",
    "    # Gaming/Entertainment\n",
    "    \"7999\",  # Online gaming platforms\n",
    "    \"7841\"   # Video streaming rental\n",
    "    \n",
    "    # Digital Services\n",
    "    \"5045\",  # Computers and software wholesale (mostly online)\n",
    "    \"5963\",  # Direct selling establishments (online)\n",
    "    \"7370\",  # Computer programming and data processing\n",
    "    \"8999\",  # Services, not elsewhere classified (many digital services)\n",
    "            \n",
    "    # E-commerce\n",
    "    \"5961\",  # Catalog and mail-order houses\n",
    "    \"5944\",  # Jewelry, watch, and silverware stores (online)\n",
    "    \"5734\",  # Computer and software stores (online)\n",
    "    \"5999\",  # Miscellaneous retail (many online)\n",
    "    \n",
    "    # Digital Education\n",
    "    \"8299\",  # Schools and educational services (online learning)\n",
    "    \n",
    "    # Digital Travel\n",
    "    \"4724\",  # Travel agencies (online booking)\n",
    "    \"4725\",  # Tour operators (online)\n",
    "    \n",
    "    # Digital Advertising\n",
    "    \"7311\",  # Advertising agencies (digital ads)\n",
    "    \"7319\",  # Advertising, not elsewhere classified\n",
    "     \n",
    "}\n",
    "\n",
    "# Define patterns that indicate online transactions in both TXN_DESCRIPTION and TERM_ADDR\n",
    "online_patterns = r\"\"\"(?i)(  # Case insensitive flag\n",
    "    www\\.|\\.com|\\.net|\\.org|\\.io|\\.co\\b|\\.ai|\\.tech|  # Common TLDs\n",
    "    online|internet|web|digital|e[\\s-]?commerce|  # Common terms\n",
    "    AliExpress|amazon|ebay|alibaba|shopify|etsy|walmart\\.com|target\\.com|  # Major e-commerce\n",
    "    Peer-to-Peer|payrange|paypal|SQ|square|venmo|apple\\s?pay|google\\s?pay|zelle|  # Payment processors\n",
    "    cloud|streaming|app\\b|api\\b|saas|platform|# Tech terms\n",
    "    FACEBK|facebook|TWITTER|LinkedIn|TikTok|OnlyFans|Patreon|GoFundMe| # social networks\n",
    "    zoom|webex|gotomeeting|teams|skype|  # Video conferencing\n",
    "    uber|lyft|doordash|grubhub|instacart|  # On-demand services\n",
    "    twitch|netflix|hulu|HLU|disney\\+|hbo\\s?max|peacock|  # Streaming\n",
    "    spotify|apple\\s?music|pandora|  # Music streaming\n",
    "    audible|kindle|  # Digital books\n",
    "    netmarble|Blizzard|steam|epic\\s?games|playstation\\s?network|xbox\\s?live|  # Gaming\n",
    "    dropbox|google\\s?drive|onedrive|box\\s?dot\\s?com|  # Cloud storage\n",
    "    quickbooks\\s?online|xero|freshbooks|  # Online accounting\n",
    "    salesforce|hubspot|zoho|  # SaaS platforms\n",
    "    airbnb|vrbo|expedia|priceline|  # Online travel\n",
    "    \\*ecomm\\*|\\*online\\*|\\*digital\\*  # Common asterisk patterns\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# logic to check both description and terminal address\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Online_status\",\n",
    "    when(\n",
    "        (regexp_extract(col(\"TXN_DESCRIPTION\"), r\"\\*\", 0) != \"\") |  # Asterisk in description\n",
    "        (col(\"SICSUBCD\").isin(online_sic_codes)) |  # Online SIC code\n",
    "        (regexp_extract(lower(col(\"TXN_DESCRIPTION\")), online_patterns, 0) != \"\") |  # Patterns in description\n",
    "        (regexp_extract(lower(col(\"TERM_ADDR\")), online_patterns, 0) != \"\"),  # New: Patterns in terminal address\n",
    "        1\n",
    "    ).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d25e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract State and Country from TXN_DESCRIPTION with validation\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"state_country\",\n",
    "    regexp_extract(trim(col(\"TXN_DESCRIPTION\")), r'.*\\s{2,}([A-Za-z]{4})$', 1)\n",
    ")\n",
    "\n",
    "# Split and validate State (first 2 chars)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"State\",\n",
    "    when(\n",
    "        upper(substring(col(\"state_country\"), 1, 2)).isin(list(valid_states) + list(valid_provinces)),\n",
    "        upper(substring(col(\"state_country\"), 1, 2))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Split and validate Country (last 2 chars)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Country\",\n",
    "    when(\n",
    "        upper(substring(col(\"state_country\"), 3, 2)).isin(list(valid_countries)),\n",
    "        upper(substring(col(\"state_country\"), 3, 2))\n",
    "    ).otherwise(None)\n",
    ").drop(\"state_country\")\n",
    "\n",
    "# Additional validation for US states\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"State\",\n",
    "    when(\n",
    "        (col(\"Country\") == \"US\") & \n",
    "        (~col(\"State\").isin(list(valid_states))),\n",
    "        None\n",
    "    ).otherwise(col(\"State\"))\n",
    ")\n",
    "\n",
    "# Additional validation for CA provinces\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"State\",\n",
    "    when(\n",
    "        (col(\"Country\") == \"CA\") & \n",
    "        (~col(\"State\").isin(list(valid_provinces))),\n",
    "        None\n",
    "    ).otherwise(col(\"State\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e6aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Company Name Extraction with Fortune 1000 matching\n",
    "\n",
    "# 4.1. Clean Fortune table data (preserve original case)\n",
    "fortune_df = spark.read.csv(fortune_path, header=True) \\\n",
    "    .select(\n",
    "        trim(col(\"Company\")).alias(\"fortune_company\"),  # Keep original case\n",
    "        trim(col(\"Alt_name\")).alias(\"fortune_alt_name\")  # Keep original case\n",
    "    ) \\\n",
    "    .filter(col(\"fortune_company\").isNotNull()) \\\n",
    "    .distinct()\n",
    "\n",
    "# 4.2. Merchant name extraction function with title case conversion\n",
    "def extract_merchant_name(txn_description):\n",
    "    \"\"\"Extracts the likely merchant name and converts to title case\"\"\"\n",
    "    if not txn_description:\n",
    "        return None\n",
    "    \n",
    "    # Remove everything after double spaces (state/country info)\n",
    "    name = re.sub(r\"\\s{2,}.*\", \"\", str(txn_description))\n",
    "    \n",
    "    # Remove common suffixes and special characters (but keep dots)\n",
    "    name = re.sub(r\"\\b(FACEBK|AMZ|TST|ABC|GOOGLE|SQ|PAYPAL|LLC|INC|LTD|CORP|CO|#\\d+|\\*\\w+)\\b|[^A-Za-z0-9.\\s]\", \"\", name)\n",
    "    \n",
    "    # Convert to title case (first letter uppercase, rest lowercase)\n",
    "    name = ' '.join([w.capitalize() for w in name.split()])\n",
    "    \n",
    "    # Take first 3 words\n",
    "    return ' '.join(name.split()[:3]).strip()\n",
    "\n",
    "extract_merchant_udf = udf(extract_merchant_name, StringType())\n",
    "\n",
    "# 4.3. Fortune matching function (case-insensitive comparison)\n",
    "def match_fortune_company(merchant_name, fortune_broadcast):\n",
    "    \"\"\"Matches against Fortune table (case-insensitive) but returns proper case\"\"\"\n",
    "    if not merchant_name:\n",
    "        return None\n",
    "    \n",
    "    merchant_name_upper = str(merchant_name).upper()\n",
    "    \n",
    "    for row in fortune_broadcast.value:\n",
    "        # 1. Check exact match with company name (case-insensitive)\n",
    "        if row['fortune_company'] and merchant_name_upper == row['fortune_company'].upper():\n",
    "            return row['fortune_company']  # Return original case from Fortune table\n",
    "        \n",
    "        # 2. Check exact match with alt name (case-insensitive)\n",
    "        if row['fortune_alt_name'] and merchant_name_upper == row['fortune_alt_name'].upper():\n",
    "            return row['fortune_company']  # Return company name in original case\n",
    "        \n",
    "        # 3. Check if merchant name starts with company name (case-insensitive)\n",
    "        if row['fortune_company'] and merchant_name_upper.startswith(row['fortune_company'].upper()):\n",
    "            return row['fortune_company']\n",
    "        \n",
    "        # 4. Check if merchant name starts with alt name (case-insensitive)\n",
    "        if row['fortune_alt_name'] and merchant_name_upper.startswith(row['fortune_alt_name'].upper()):\n",
    "            return row['fortune_company']\n",
    "    \n",
    "    return None\n",
    "\n",
    "fortune_broadcast = spark.sparkContext.broadcast(fortune_df.collect())\n",
    "fortune_match_udf = udf(lambda x: match_fortune_company(x, fortune_broadcast), StringType())\n",
    "\n",
    "# 4.4. Apply to dataframe\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Cleaned_Merchant\",\n",
    "    extract_merchant_udf(col(\"TXN_DESCRIPTION\"))\n",
    ")\n",
    "\n",
    "# First try matching with Fortune table\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Fortune_Match\",\n",
    "    fortune_match_udf(col(\"Cleaned_Merchant\"))\n",
    ")\n",
    "\n",
    "# Final Parent_Company: Use Fortune match if found (with original case), \n",
    "# otherwise use cleaned merchant name (in title case)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Parent_Company\",\n",
    "    coalesce(col(\"Fortune_Match\"), col(\"Cleaned_Merchant\"))\n",
    ")\n",
    "\n",
    "# 4.5. Add Parent_Company_Flag\n",
    "\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Parent_Company_Flag\",\n",
    "    when(\n",
    "        col(\"Fortune_Match\").isNotNull(),  # If we got a match from Fortune table\n",
    "        1\n",
    "    ).when(\n",
    "        col(\"Parent_Company\").isNotNull(),  # If we have a parent company but not from Fortune\n",
    "        0\n",
    "    ).otherwise(  # If Parent_Company is null\n",
    "        None\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "merged_df = merged_df.drop(\"Fortune_Match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce659812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6898d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample results\n",
    "#merged_df.select(\"TXN_DESCRIPTION\", \"Parent_Company\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71197c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Enhanced City Extraction from TXN_DESCRIPTION (Handles multi-word cities)\n",
    "valid_cities = cities_df.select(\n",
    "    upper(col(\"city\")).alias(\"city\"),\n",
    "    col(\"country_id\")\n",
    ").filter(col(\"city\").isNotNull()).collect()\n",
    "\n",
    "# Create a dictionary of cities by country, sorted by length (longest first)\n",
    "valid_cities_dict = {}\n",
    "for row in valid_cities:\n",
    "    country = row[\"country_id\"]\n",
    "    city = row[\"city\"]\n",
    "    if country not in valid_cities_dict:\n",
    "        valid_cities_dict[country] = []\n",
    "    valid_cities_dict[country].append(city)\n",
    "\n",
    "# Sort city lists by length (longest first) to prioritize multi-word matches\n",
    "for country in valid_cities_dict:\n",
    "    valid_cities_dict[country].sort(key=len, reverse=True)\n",
    "\n",
    "def extract_city(txn_description, country, parent_company, online_status):\n",
    "    if online_status == 1:\n",
    "        return \"Online\"\n",
    "    if txn_description is None or country is None:\n",
    "        return None\n",
    "    \n",
    "    country = str(country).upper()\n",
    "    txn_description = str(txn_description).strip()\n",
    "    \n",
    "    # Remove parent company name if present\n",
    "    if parent_company is not None:\n",
    "        parent_company = str(parent_company)\n",
    "        txn_description = txn_description.replace(parent_company, '').strip()\n",
    "    \n",
    "    # Remove state/country suffix if present (e.g., \"MNUS\")\n",
    "    state_country_pattern = r\"[A-Z]{2}[A-Z]{2}$\"\n",
    "    txn_description = re.sub(state_country_pattern, \"\", txn_description).strip()\n",
    "    \n",
    "    # Get valid cities for this country (already sorted longest first)\n",
    "    country_cities = valid_cities_dict.get(country, [])\n",
    "    \n",
    "    # Try to find the longest possible city name match first\n",
    "    for city in country_cities:\n",
    "        # Look for city at end of string (after stripping whitespace)\n",
    "        if txn_description.rstrip().endswith(city):\n",
    "            return city\n",
    "        \n",
    "        # Look for city followed by multiple spaces (common pattern)\n",
    "        if f\"{city}  \" in txn_description:\n",
    "            return city\n",
    "    \n",
    "    # Fallback: Split by multiple spaces and check last components\n",
    "    parts = [p.strip() for p in re.split(r\"\\s{2,}\", txn_description) if p.strip()]\n",
    "    if parts:\n",
    "        # Check last part (most likely city location)\n",
    "        last_part = parts[-1].upper()\n",
    "        for city in country_cities:\n",
    "            if last_part == city:\n",
    "                return city\n",
    "            if last_part.endswith(city):\n",
    "                return city\n",
    "    \n",
    "    return None\n",
    "\n",
    "extract_city_udf = F.udf(extract_city, StringType())\n",
    "\n",
    "# Apply city extraction first\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"City\",\n",
    "    extract_city_udf(\n",
    "        col(\"TXN_DESCRIPTION\"),\n",
    "        col(\"Country\"), \n",
    "        col(\"Parent_Company\"),\n",
    "        col(\"Online_status\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6962fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 10208188\n"
     ]
    }
   ],
   "source": [
    "# 6. Address Extraction\n",
    "def extract_address_only(txn_description, parent_company, city, state, country):\n",
    "    if txn_description is None or state is None or country is None:\n",
    "        return None\n",
    "    \n",
    "    txn_description = txn_description.strip()\n",
    "    state_country = f\"{state}{country}\"\n",
    "    \n",
    "    if state_country in txn_description:\n",
    "        remaining = txn_description.replace(state_country, '').strip()\n",
    "    else:\n",
    "        remaining = txn_description\n",
    "    \n",
    "    if parent_company is not None:\n",
    "        remaining = remaining.replace(str(parent_company), '').strip()\n",
    "    \n",
    "    if city is not None:\n",
    "        remaining = remaining.replace(str(city), '').strip()\n",
    "    \n",
    "    return remaining if remaining else None\n",
    "\n",
    "extract_address_only_udf = F.udf(extract_address_only, StringType())\n",
    "\n",
    "# 6.1 Identification Flag\n",
    "def validate_address(address):\n",
    "    if address is None or not isinstance(address, str):\n",
    "        return 0\n",
    "    address_suffixes = r'\\b(AV|AVE|ST|EASTERN|Street|RD|Road|BLVD|Boulevard|DR|Drive|LN|Lane|South|North|East|West|S|N|E|W)\\b'\n",
    "    return 1 if re.search(address_suffixes, address, flags=re.IGNORECASE) else 0\n",
    "\n",
    "validate_address_udf = F.udf(validate_address, IntegerType())\n",
    "\n",
    "# Apply both stages sequentially\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Address_Candidate\",\n",
    "    extract_address_only_udf(\n",
    "        col(\"TXN_DESCRIPTION\"),\n",
    "        col(\"Parent_Company\"), \n",
    "        col(\"City\"),\n",
    "        col(\"State\"),\n",
    "        col(\"Country\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Then validate and flag\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Address_Flag\",\n",
    "    validate_address_udf(col(\"Address_Candidate\"))\n",
    ")\n",
    "\n",
    "# Final clean address (None if not valid)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Address\",\n",
    "    when(col(\"Address_Flag\") == 1, col(\"Address_Candidate\")).otherwise(None)\n",
    ")\n",
    "\n",
    "# Drop the temporary candidate column\n",
    "merged_df = merged_df.drop(\"Address_Candidate\")\n",
    "\n",
    "# Verify the Address column was created\n",
    "merged_df = merged_df.withColumn(\"Address_debug\", \n",
    "    when(col(\"Address\").isNotNull(), lit(\"Valid\")))\n",
    "    \n",
    "print(f\"Total rows: {merged_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ac0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Extract State from TERM_ADDR\n",
    "merged_df = (\n",
    "    merged_df\n",
    "    # Extract first 2 characters (uppercased)\n",
    "    .withColumn(\n",
    "        \"TERM_State_Raw\",\n",
    "        upper(substring(trim(col(\"TERM_ADDR\")), 1, 2))\n",
    "    )\n",
    "    \n",
    "    # Validate against states/provinces\n",
    "    .withColumn(\n",
    "        \"TERM_State\",\n",
    "        when(\n",
    "            col(\"TERM_State_Raw\").isin(list(valid_states) + list(valid_provinces)),\n",
    "            col(\"TERM_State_Raw\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Additional country-specific validation\n",
    "    .withColumn(\n",
    "        \"TERM_State\",\n",
    "        when(\n",
    "            (col(\"Country\") == \"US\") & \n",
    "            (~col(\"TERM_State\").isin(list(valid_states))),\n",
    "            None\n",
    "        )\n",
    "        .when(\n",
    "            (col(\"Country\") == \"CA\") & \n",
    "            (~col(\"TERM_State\").isin(list(valid_provinces))),\n",
    "            None\n",
    "        )\n",
    "        .otherwise(col(\"TERM_State\"))\n",
    "    )\n",
    "    \n",
    "    # Cleanup temporary column\n",
    "    .drop(\"TERM_State_Raw\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "664efa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Extract City from TERM_ADDR\n",
    "state_column_name = 'state_id'\n",
    "\n",
    "us_cities = cities_df.filter(col(\"country_id\") == \"US\") \\\n",
    "    .select(\n",
    "        upper(col(\"city\")).alias(\"city\"),\n",
    "        col(state_column_name)\n",
    "    ).filter(col(\"city\").isNotNull()).collect()\n",
    "\n",
    "valid_cities_by_state = {}\n",
    "for row in us_cities:\n",
    "    state = row[state_column_name]\n",
    "    city = row[\"city\"]\n",
    "    if state not in valid_cities_by_state:\n",
    "        valid_cities_by_state[state] = set()\n",
    "    valid_cities_by_state[state].add(city)\n",
    "\n",
    "def extract_term_city(term_addr, term_state):\n",
    "    if term_addr is None or term_state is None:\n",
    "        return None\n",
    "    \n",
    "    term_state = str(term_state).upper()\n",
    "    term_addr = str(term_addr).strip().upper()\n",
    "    state_cities = valid_cities_by_state.get(term_state, set())\n",
    "    \n",
    "    space_indices = [m.start() for m in re.finditer(r'  +', term_addr)]\n",
    "    \n",
    "    if not space_indices:\n",
    "        for city in state_cities:\n",
    "            city = str(city)\n",
    "            if city in term_addr:\n",
    "                return city\n",
    "        return None\n",
    "    \n",
    "    if len(space_indices) == 1:\n",
    "        city_candidate = term_addr[:space_indices[0]].strip()\n",
    "        city_candidate = str(city_candidate)\n",
    "        for city in state_cities:\n",
    "            city = str(city)\n",
    "            if city in city_candidate:\n",
    "                return city\n",
    "        \n",
    "        for city in state_cities:\n",
    "            city = str(city)\n",
    "            if city in term_addr:\n",
    "                return city\n",
    "    \n",
    "    else:\n",
    "        city_candidate = term_addr[space_indices[0]:space_indices[1]].strip()\n",
    "        city_candidate = str(city_candidate)\n",
    "        for city in state_cities:\n",
    "            city = str(city)\n",
    "            if city in city_candidate:\n",
    "                return city\n",
    "    \n",
    "    return None\n",
    "\n",
    "extract_term_city_udf = F.udf(extract_term_city, StringType())\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"TERM_City\",\n",
    "    extract_term_city_udf(\n",
    "        col(\"TERM_ADDR\"),\n",
    "        col(\"TERM_State\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0c1b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Extract Address from TERM_ADDR (State first, then City)\n",
    "from pyspark.sql.functions import regexp_replace, trim, when, col, expr\n",
    "\n",
    "# First create a cleaned version\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"term_street\", \n",
    "    trim(col(\"TERM_ADDR\"))\n",
    ")\n",
    "\n",
    "# Remove phone numbers in formats like 307-222-0026 or (307) 222-0026\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"term_street\",\n",
    "    regexp_replace(\n",
    "        col(\"term_street\"),\n",
    "        r\"\\b(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\\b\",\n",
    "        \"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Handle cases where both state and city exist in order: STATE CITY STREET\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"term_street\",\n",
    "    when(\n",
    "        (col(\"TERM_State\").isNotNull() & col(\"TERM_City\").isNotNull()),\n",
    "        expr(\"\"\"\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    term_street,\n",
    "                    concat('^\\\\\\\\s*', TERM_State, '\\\\\\\\s+', TERM_City, '\\\\\\\\s+'),\n",
    "                    ''\n",
    "                ),\n",
    "                concat('^\\\\\\\\s*', TERM_State, '\\\\\\\\s*$'),\n",
    "                ''\n",
    "            )\n",
    "        \"\"\")\n",
    "    ).otherwise(col(\"term_street\"))\n",
    ")\n",
    "\n",
    "# Handle cases where only state exists\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"term_street\",\n",
    "    when(\n",
    "        (col(\"TERM_State\").isNotNull() & col(\"TERM_City\").isNull()),\n",
    "        expr(\"regexp_replace(term_street, concat('^\\\\\\\\s*', TERM_State, '\\\\\\\\s+'), '')\")\n",
    "    ).otherwise(col(\"term_street\"))\n",
    ")\n",
    "\n",
    "# Final cleanup - remove extra spaces and trim\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"term_street\",\n",
    "    trim(regexp_replace(col(\"term_street\"), r'\\\\s{2,}', ' '))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29c3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Validation flag\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"term_street_flag\",\n",
    "    when(\n",
    "        (col(\"TERM_State\").isNotNull()) & \n",
    "        (col(\"TERM_City\").isNotNull()) & \n",
    "        (col(\"term_street\").isNotNull()) & \n",
    "        (col(\"term_street\") != \"\"),\n",
    "        1  # Valid if all three fields exist\n",
    "    ).otherwise(0)  # Otherwise, flag as 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a56fb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create Company column by combining Parent_Company + Address + City + State (skip nulls)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Company\",\n",
    "    concat_ws(\" \",\n",
    "        *[when(col(c).isNotNull(), col(c)).otherwise(\"\") \n",
    "         for c in [\"Parent_Company\", \"Address\", \"City\", \"State\"]]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean up the Company column to remove extra spaces\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Company\",\n",
    "    regexp_replace(trim(col(\"Company\")), r\" +\", \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4ac217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Unique Company Key Generation\n",
    "from pyspark.sql.functions import col, when, lit, concat_ws, upper, regexp_replace, coalesce, regexp_extract, substring\n",
    "\n",
    "# Step 1: Clean and standardize company name\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"clean_company\",\n",
    "    coalesce(\n",
    "        upper(trim(col(\"Parent_Company\"))),  # rim and uppercase\n",
    "        lit(\"UNKNOWN\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Extract address component\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"clean_address_part\",\n",
    "    when(\n",
    "        col(\"Address\").isNotNull(),\n",
    "        upper(\n",
    "            coalesce(\n",
    "                regexp_extract(col(\"Address\"), r\".*?(\\d+).*\", 1),  # Extract ANY digits\n",
    "                regexp_extract(col(\"Address\"), r\"^(\\w+)\", 1),       # Fallback: first word\n",
    "                lit(\"\")\n",
    "            )\n",
    "        )\n",
    "    ).otherwise(lit(\"NA\"))\n",
    ")\n",
    "\n",
    "# Step 3: Build key components with validation\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"merchant_component\",\n",
    "    coalesce(col(\"HASHED_MERCH_ID\"), lit(\"NA\"))\n",
    ")\n",
    "\n",
    "# Location-based key (for physical stores)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"location_component\",\n",
    "    when(\n",
    "        (col(\"Address_Flag\") == 1) &\n",
    "        (col(\"City\").isNotNull()) &\n",
    "        (col(\"State\").isNotNull()),\n",
    "        concat_ws(\"_\",\n",
    "            col(\"clean_company\"),\n",
    "            substring(col(\"clean_address_part\"), 1, 10),  # Use first 10 chars of address part\n",
    "            upper(regexp_replace(col(\"City\"), r\"[^A-Z]\", \"\")),\n",
    "            col(\"State\")\n",
    "        )\n",
    "    ).otherwise(lit(\"NA\"))\n",
    ")\n",
    "\n",
    "# Online key (for e-commerce/online transactions)\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"online_component\",\n",
    "    when(\n",
    "        col(\"Online_status\") == 1,\n",
    "        concat_ws(\"_\",\n",
    "            col(\"clean_company\"),\n",
    "            lit(\"ONLINE\"),\n",
    "            coalesce(col(\"Country\"), lit(\"UNK\"))\n",
    "        )\n",
    "    ).otherwise(lit(\"NA\"))\n",
    ")\n",
    "\n",
    "# Step 4: Combine into final composite key\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"COMPANY_KEY\",\n",
    "    concat_ws(\"|\",\n",
    "        col(\"merchant_component\"),\n",
    "        col(\"location_component\"),\n",
    "        col(\"online_component\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 5: Cleanup temporary columns\n",
    "merged_df = merged_df.drop(\n",
    "    \"clean_company\", \"clean_address_part\",\n",
    "    \"merchant_component\", \"location_component\", \"online_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb9c6cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset columns:\n",
      "['SICSUBCD', 'TXN_DESCRIPTION', 'TERM_ADDR', 'HASHED_MERCH_ID', 'Related_SIC_Code_Description', 'Online_status', 'State', 'Country', 'Cleaned_Merchant', 'Parent_Company', 'Parent_Company_Flag', 'City', 'Address_Flag', 'Address', 'Address_debug', 'TERM_State', 'TERM_City', 'term_street', 'term_street_flag', 'Company', 'COMPANY_KEY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[SICSUBCD: string, TXN_DESCRIPTION: string, TERM_ADDR: string, HASHED_MERCH_ID: string, Related SIC Code Description: string, Online_status: int, State: string, Country: string, Cleaned_Merchant: string, Parent_Company: string, Parent_Company_Flag: int, City: string, Address_Flag: int, Address: string, Address_debug: string, TERM_State: string, TERM_City: string, term_street: string, term_street_flag: int, Company: string, COMPANY_KEY: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12. Final Cleanup and Output\n",
    "# First ensure we have the term_street column\n",
    "if 'term_street' not in merged_df.columns:\n",
    "    merged_df = merged_df.withColumn(\n",
    "        \"term_street\",\n",
    "        when(col(\"TERM_State\").isNotNull(),\n",
    "            regexp_replace(\n",
    "                regexp_extract(col(\"TERM_ADDR\"), r'^(.*?)(?=\\\\\\\\s+[A-Z]{2}\\\\\\\\s+[A-Z]{2}$)', 0),\n",
    "                r'\\\\\\\\d+$', ''\n",
    "            )\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "\n",
    "# Cache DataFrame for performance\n",
    "merged_df.cache()\n",
    "\n",
    "# Add match quality flag based on extracted components only\n",
    "validated_df = merged_df.withColumn(\n",
    "    \"term_street_flag\",\n",
    "    when(col(\"TERM_State\").isNull(), 0)  # No state extracted\n",
    "    .when(col(\"term_street\").isNull(), 1)  # State but no address extracted\n",
    "    .otherwise(2)  # Valid address components extracted\n",
    ")\n",
    "\n",
    "# Create final output DataFrame\n",
    "#final_df = validated_df.drop(\"term_street\")\n",
    "final_df = validated_df.cache()\n",
    "\n",
    "# Clean column names\n",
    "for col_name in final_df.columns:\n",
    "    clean_name = (col_name.replace(\" \", \"_\")\n",
    "                        .replace(\"-\", \"_\")\n",
    "                        .replace(\"(\", \"\")\n",
    "                        .replace(\")\", \"\")\n",
    "                        .replace(\"%\", \"pct\")\n",
    "                        .replace(\".\", \"_\"))\n",
    "    final_df = final_df.withColumnRenamed(col_name, clean_name)\n",
    "\n",
    "print(\"\\nFinal dataset columns:\")\n",
    "print(final_df.columns)\n",
    "\n",
    "# Clean up cached DataFrame\n",
    "merged_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba7ed16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset also saved to CSV: /mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Agustin/Gate_City_processed_completed_TEST.csv\n"
     ]
    }
   ],
   "source": [
    "# 13. Save the final processed dataset\n",
    "# First drop innecesary columns\n",
    "merged_df = merged_df.drop(\"Address_debug\",\"Cleaned_Merchant\")\n",
    "\n",
    "for col_name in merged_df.columns:\n",
    "    clean_name = (col_name.replace(\" \", \"_\")\n",
    "                        .replace(\"-\", \"_\")\n",
    "                        .replace(\"(\", \"\")\n",
    "                        .replace(\")\", \"\")\n",
    "                        .replace(\"%\", \"pct\")\n",
    "                        .replace(\".\", \"_\"))\n",
    "    merged_df = merged_df.withColumnRenamed(col_name, clean_name)\n",
    "    \n",
    "#merged_df.show()\n",
    "\n",
    "output_path = '/mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Agustin/Gate_City_processed_completed_TEST.csv'\n",
    "#output_path = '/mmfs1/home/a.patronigranda/MIS795 Capstone Project/Output'\n",
    "merged_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "print(f\"Final dataset also saved to CSV: {output_path}\")\n",
    "\n",
    "# Optional: Save as Parquet\n",
    "#parquet_path = '/mmfs1/projects/f8d7c0/2024-25/Gate City Bank/Agustin/Gate_City_processed_completed.parquet'\n",
    "#merged_df.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "#print(f\"Final dataset saved as Parquet to: {parquet_path}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588846c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
